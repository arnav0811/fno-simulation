{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jMOqIsUof7s6"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install h5py matplotlib numpy scipy tqdm einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "import os\n",
        "if not os.path.exists('PDEBench'):\n",
        "    !git clone https://github.com/pdebench/PDEBench.git\n",
        "%cd PDEBench\n",
        "# Relax torchvision and torch requirements in pyproject.toml to match the installed version (compatible with Python 3.12)\n",
        "!sed -i 's/torchvision~=0.14.1/torchvision/' pyproject.toml\n",
        "!sed -i 's/torch~=1.13.0/torch/' pyproject.toml\n",
        "# Relax Python version requirement\n",
        "!sed -i 's/requires-python = .*/requires-python = \">=3.9\"/' pyproject.toml\n",
        "!pip install -e ."
      ],
      "metadata": {
        "collapsed": true,
        "id": "WylDZBV68sq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import h5py\n",
        "\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Load the URLs CSV to find the correct link\n",
        "urls_df = pd.read_csv('/content/PDEBench/pdebench/data_download/pdebench_data_urls.csv')\n",
        "\n",
        "# Search for the 2D Diffusion-Reaction dataset\n",
        "file_row = urls_df[urls_df['Filename'] == '2D_diff-react_NA_NA.h5']\n",
        "\n",
        "if not file_row.empty:\n",
        "    DATASET_URL = file_row.iloc[0]['URL']\n",
        "    DATASET_PATH = os.path.join('data', file_row.iloc[0]['Filename'])\n",
        "\n",
        "    print(f\"Found URL: {DATASET_URL}\")\n",
        "    print(f\"Target Path: {DATASET_PATH}\")\n",
        "\n",
        "    def download_dataset():\n",
        "        print(\"Downloading 2D Diffusion-Reaction dataset...\")\n",
        "        class DownloadProgressBar(tqdm):\n",
        "            def update_to(self, b=1, bsize=1, tsize=None):\n",
        "                if tsize is not None:\n",
        "                    self.total = tsize\n",
        "                self.update(b * bsize - self.n)\n",
        "\n",
        "        with DownloadProgressBar(unit='B', unit_scale=True, miniters=1, desc=\"Downloading\") as t:\n",
        "            urllib.request.urlretrieve(DATASET_URL, DATASET_PATH, reporthook=t.update_to)\n",
        "        print(f\"Downloaded to {DATASET_PATH}\")\n",
        "\n",
        "    # Check if file exists and is valid\n",
        "    if os.path.exists(DATASET_PATH):\n",
        "        try:\n",
        "            # Try opening the file to check for corruption\n",
        "            with h5py.File(DATASET_PATH, 'r') as f:\n",
        "                pass\n",
        "            print(f\"Dataset exists and is valid at {DATASET_PATH}\")\n",
        "        except OSError:\n",
        "            print(\"Detected corrupted/incomplete file. Deleting and re-downloading...\")\n",
        "            os.remove(DATASET_PATH)\n",
        "            download_dataset()\n",
        "    else:\n",
        "        download_dataset()\n",
        "\n",
        "    # Proceed with inspecting the dataset\n",
        "    with h5py.File(DATASET_PATH, 'r') as f:\n",
        "        print(f\"\\nDataset structure:\")\n",
        "        # Recursively visit items to handle Groups\n",
        "        def print_item(name, obj):\n",
        "            if isinstance(obj, h5py.Dataset):\n",
        "                print(f\"  {name}: {obj.shape}\")\n",
        "            else:\n",
        "                print(f\"  {name}/\")\n",
        "\n",
        "        f.visititems(print_item)\n",
        "\n",
        "else:\n",
        "    print(\"Could not find the dataset URL in the metadata. Please check the 'urls_df' manually.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "l7p8F_e48zQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a0a0790"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "842350af"
      },
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "with h5py.File(DATASET_PATH, 'r') as f:\n",
        "    keys = sorted(list(f.keys()))\n",
        "    print(f\"Root keys (first 5): {keys[:5]}\")\n",
        "\n",
        "    # Check if 'data' is the main key or if we have sample groups\n",
        "    if 'data' in f:\n",
        "        # Lazy load the dataset (don't read into RAM yet)\n",
        "        data_dset = f['data']\n",
        "        print(f\"Data shape: {data_dset.shape}\")\n",
        "        # Load sample 0\n",
        "        sample_data = data_dset[0]\n",
        "        num_samples = data_dset.shape[0]\n",
        "    else:\n",
        "        # Assume structure is Group '0000' -> Dataset 'data'\n",
        "        # Inspect first group\n",
        "        first_group = f[keys[0]]\n",
        "        if 'data' in first_group:\n",
        "            # Shape of one sample: [time, x, y, c]\n",
        "            sample_shape = first_group['data'].shape\n",
        "            num_samples = len(keys)\n",
        "            print(f\"Data appears to be split across {num_samples} groups.\")\n",
        "            print(f\"Single sample shape: {sample_shape}\")\n",
        "\n",
        "            # Load sample 0\n",
        "            sample_data = first_group['data'][:]\n",
        "        else:\n",
        "             raise ValueError(f\"Could not find 'data' dataset in group {keys[0]}. Keys: {first_group.keys()}\")\n",
        "\n",
        "    print(f\"  - Samples: {num_samples}\")\n",
        "    print(f\"  - Time steps: {sample_data.shape[0]}\")\n",
        "    print(f\"  - Spatial: {sample_data.shape[1]} x {sample_data.shape[2]}\")\n",
        "    print(f\"  - Channels (u, v): {sample_data.shape[3]}\")\n",
        "\n",
        "    # Visualize a sample\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "\n",
        "    # Select 5 evenly spaced time steps\n",
        "    time_steps = np.linspace(0, sample_data.shape[0]-1, 5, dtype=int)\n",
        "\n",
        "    for i, t in enumerate(time_steps):\n",
        "        # Activator (u)\n",
        "        axes[0, i].imshow(sample_data[t, :, :, 0], cmap='viridis')\n",
        "        axes[0, i].set_title(f't={t}')\n",
        "        axes[0, i].axis('off')\n",
        "        if i == 0:\n",
        "            axes[0, i].set_ylabel('Activator (u)')\n",
        "\n",
        "        # Inhibitor (v)\n",
        "        axes[1, i].imshow(sample_data[t, :, :, 1], cmap='plasma')\n",
        "        axes[1, i].axis('off')\n",
        "        if i == 0:\n",
        "            axes[1, i].set_ylabel('Inhibitor (v)')\n",
        "\n",
        "    plt.suptitle('2D Diffusion-Reaction: Time Evolution', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('data_visualization.png', dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\nThis is what we're learning to predict:\")\n",
        "print(\"Given initial state (t=0), predict future states (t=1,2,...,T)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69a8c033"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Success! GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "    print(\"You can now re-run the notebook cells to install libraries and download the data.\")\n",
        "else:\n",
        "    print(\"WARNING: GPU not detected. Please check 'Runtime > Change runtime type' and select T4 GPU.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import h5py\n",
        "\n",
        "class LazyDiffusionReactionDataset(Dataset):\n",
        "    def __init__(self, file_path, split='train', train_ratio=0.8):\n",
        "        self.file_path = file_path\n",
        "        self.split = split\n",
        "\n",
        "        # Open file just to read metadata\n",
        "        with h5py.File(file_path, 'r') as f:\n",
        "            self.keys = sorted(list(f.keys()))\n",
        "            # Filter for actual data groups (assuming numbered keys like '0000')\n",
        "            self.keys = [k for k in self.keys if k.isdigit()]\n",
        "\n",
        "            # Split into train/test\n",
        "            n_train = int(len(self.keys) * train_ratio)\n",
        "            if split == 'train':\n",
        "                self.keys = self.keys[:n_train]\n",
        "            else:\n",
        "                self.keys = self.keys[n_train:]\n",
        "\n",
        "            # Get dimensions from first sample\n",
        "            sample0 = f[self.keys[0]]['data']\n",
        "            self.time_steps = sample0.shape[0] - 1 # predicting t -> t+1\n",
        "\n",
        "        print(f\"[{split}] Initialized with {len(self.keys)} trajectories\")\n",
        "\n",
        "    def __len__(self):\n",
        "        # Total samples = (Number of trajectories) * (Time steps per trajectory)\n",
        "        return len(self.keys) * self.time_steps\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Map flat index to (trajectory_id, time_step)\n",
        "        traj_idx = idx // self.time_steps\n",
        "        t_idx = idx % self.time_steps\n",
        "\n",
        "        key = self.keys[traj_idx]\n",
        "\n",
        "        # Open file ONLY for this specific read (saves RAM)\n",
        "        with h5py.File(self.file_path, 'r') as f:\n",
        "            # Read only the two necessary frames (t and t+1)\n",
        "            # Slicing [t_idx : t_idx+2] reads just 2 frames, not the whole video\n",
        "            frames = f[key]['data'][t_idx : t_idx+2]\n",
        "\n",
        "        x = frames[0] # Input (t)\n",
        "        y = frames[1] # Target (t+1)\n",
        "\n",
        "        # Convert to Torch: [H, W, C] -> [C, H, W]\n",
        "        x = torch.from_numpy(x).permute(2, 0, 1).float()\n",
        "        y = torch.from_numpy(y).permute(2, 0, 1).float()\n",
        "\n",
        "        return x, y\n",
        "\n",
        "# Initialize the lazy loaders\n",
        "# Note: num_workers=0 is safer for HDF5 files to avoid read conflicts\n",
        "train_dataset = LazyDiffusionReactionDataset(DATASET_PATH, split='train')\n",
        "test_dataset = LazyDiffusionReactionDataset(DATASET_PATH, split='test')\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
        "\n",
        "# Verify one batch\n",
        "x, y = next(iter(train_loader))\n",
        "print(f\"\\nBatch Loaded Successfully!\")\n",
        "print(f\"Input Shape: {x.shape} (Batch, Channels, Height, Width)\")"
      ],
      "metadata": {
        "id": "fSAz5xy4-miK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "FNO: Fourier Neural Operator\n",
        "\n",
        "Instead of learning spatial convolution kernels (like CNN),\n",
        "FNO learns the kernel directly in Fourier space. This makes it:\n",
        "1. Resolution-independent (train on 64x64, test on 256x256)\n",
        "2. Naturally captures global patterns (not just local)\n",
        "3. Efficient for smooth PDE solutions\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SpectralConv2d(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, modes1, modes2):\n",
        "    super().__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.modes1 = modes1  # Number of Fourier modes to keep (height)\n",
        "    self.modes2 = modes2  # Number of Fourier modes to keep (width)\n",
        "\n",
        "    # Scale factor for initialization\n",
        "    self.scale = 1 / (in_channels * out_channels)\n",
        "\n",
        "    # Learnable weights in Fourier space\n",
        "    # These are COMPLEX numbers (real + imaginary parts)\n",
        "    # Shape: [in_channels, out_channels, modes1, modes2]\n",
        "    self.weights1 = nn.Parameter(\n",
        "      self.scale * torch.rand(in_channels, out_channels, modes1, modes2, dtype=torch.cfloat)\n",
        "    )\n",
        "    self.weights2 = nn.Parameter(\n",
        "      self.scale * torch.rand(in_channels, out_channels, modes1, modes2, dtype=torch.cfloat)\n",
        "    )\n",
        "\n",
        "  def compl_mul2d(self, input, weights):\n",
        "      \"\"\"\n",
        "      Complex multiplication in Fourier space.\n",
        "\n",
        "      input: [batch, in_channels, height, width] (complex)\n",
        "      weights: [in_channels, out_channels, height, width] (complex)\n",
        "      output: [batch, out_channels, height, width] (complex)\n",
        "\n",
        "      Einstein notation: batch(b), in_channel(i), out_channel(o), x, y\n",
        "      \"\"\"\n",
        "      return torch.einsum(\"bixy,ioxy->boxy\", input, weights)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    # FFT (spatial domain → frequency domain)\n",
        "    # rfft2 = real FFT for 2D (more efficient than full FFT for real inputs)\n",
        "    x_ft = torch.fft.rfft2(x)\n",
        "\n",
        "    # Multiply by learnable weights in Fourier space\n",
        "    # We only keep low-frequency modes (truncate high frequencies)\n",
        "    # This is like a learnable low-pass filter\n",
        "    out_ft = torch.zeros(\n",
        "      batch_size, self.out_channels, x.size(-2), x.size(-1) // 2 + 1,\n",
        "      dtype=torch.cfloat, device=x.device\n",
        "    )\n",
        "\n",
        "    # Lower frequencies (top-left corner in FFT output)\n",
        "    out_ft[:, :, :self.modes1, :self.modes2] = \\\n",
        "      self.compl_mul2d(x_ft[:, :, :self.modes1, :self.modes2], self.weights1)\n",
        "\n",
        "    # Higher frequencies that wrap around (bottom-left corner)\n",
        "    out_ft[:, :, -self.modes1:, :self.modes2] = \\\n",
        "      self.compl_mul2d(x_ft[:, :, -self.modes1:, :self.modes2], self.weights2)\n",
        "\n",
        "    # Inverse FFT (frequency domain → spatial domain)\n",
        "    x = torch.fft.irfft2(out_ft, s=(x.size(-2), x.size(-1)))\n",
        "\n",
        "    return x\n",
        "\n",
        "class FNO2d(nn.Module):\n",
        "  \"\"\"\n",
        "  Complete 2D Fourier Neural Operator.\n",
        "\n",
        "  Architecture:\n",
        "  1. LIFT: Project input channels to higher dimension\n",
        "  2. FOURIER LAYERS: Spectral convolution + regular convolution (residual)\n",
        "  3. PROJECT: Map back to output channels\n",
        "\n",
        "  The combination of spectral conv (global, smooth patterns) and\n",
        "  regular conv (local details) captures both large-scale and small-scale features.\n",
        "  \"\"\"\n",
        "  def __init__(\n",
        "    self,\n",
        "    modes1: int = 12,          # Fourier modes in height\n",
        "    modes2: int = 12,          # Fourier modes in width\n",
        "    width: int = 32,           # Hidden channel dimension\n",
        "    in_channels: int = 2,      # Input: u and v fields\n",
        "    out_channels: int = 2,     # Output: u and v fields (next timestep)\n",
        "    n_layers: int = 4,         # Number of Fourier layers\n",
        "  ):\n",
        "    super().__init__()\n",
        "\n",
        "    self.modes1 = modes1\n",
        "    self.modes2 = modes2\n",
        "    self.width = width\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    # LIFT: [in_channels + 2] → [width]\n",
        "    # +2 because we concatenate (x, y) coordinate grid\n",
        "    # This helps the model know WHERE it is in the domain\n",
        "    self.fc0 = nn.Linear(in_channels + 2, width)\n",
        "\n",
        "    # FOURIER LAYERS\n",
        "    self.spectral_convs = nn.ModuleList([\n",
        "      SpectralConv2d(width, width, modes1, modes2)\n",
        "      for _ in range(n_layers)\n",
        "    ])\n",
        "\n",
        "    # Regular 1x1 convolutions (local/residual path)\n",
        "    self.conv_layers = nn.ModuleList([\n",
        "      nn.Conv2d(width, width, 1)\n",
        "      for _ in range(n_layers)\n",
        "    ])\n",
        "\n",
        "    # PROJECT: [width] → [128] → [out_channels]\n",
        "    self.fc1 = nn.Linear(width, 128)\n",
        "    self.fc2 = nn.Linear(128, out_channels)\n",
        "\n",
        "  def get_grid(self, shape, device):\n",
        "      \"\"\"\n",
        "      Create normalized (x, y) coordinate grid.\n",
        "\n",
        "      This is crucial: it tells the model WHERE each point is.\n",
        "      Without this, the model can't distinguish boundaries from interior.\n",
        "\n",
        "      For batteries: boundary = cooling plate, interior = cell center\n",
        "      The model needs to know this to predict correctly.\n",
        "      \"\"\"\n",
        "      batch_size, _, size_x, size_y = shape\n",
        "\n",
        "      # Create 1D coordinates [0, 1]\n",
        "      gridx = torch.linspace(0, 1, size_x, device=device)\n",
        "      gridy = torch.linspace(0, 1, size_y, device=device)\n",
        "\n",
        "      # Create 2D meshgrid\n",
        "      gridx, gridy = torch.meshgrid(gridx, gridy, indexing='ij')\n",
        "\n",
        "      # Stack and expand for batch: [batch, height, width, 2]\n",
        "      grid = torch.stack([gridx, gridy], dim=-1)\n",
        "      grid = grid.unsqueeze(0).repeat(batch_size, 1, 1, 1)\n",
        "\n",
        "      return grid\n",
        "\n",
        "  def forward(self, x):\n",
        "      \"\"\"\n",
        "      Forward pass.\n",
        "\n",
        "      Input x: [batch, channels, height, width]\n",
        "      Output:  [batch, channels, height, width]\n",
        "      \"\"\"\n",
        "\n",
        "      # Get coordinate grid\n",
        "      grid = self.get_grid(x.shape, x.device)\n",
        "\n",
        "      # Reshape: [B, C, H, W] → [B, H, W, C]\n",
        "      x = x.permute(0, 2, 3, 1)\n",
        "\n",
        "      # Concatenate with grid coordinates: [B, H, W, C+2]\n",
        "      x = torch.cat([x, grid], dim=-1)\n",
        "\n",
        "      # LIFT to higher dimension\n",
        "      x = self.fc0(x)  # [B, H, W, width]\n",
        "      x = x.permute(0, 3, 1, 2)  # [B, width, H, W]\n",
        "\n",
        "      # FOURIER LAYERS\n",
        "      for i in range(self.n_layers):\n",
        "          # Two parallel paths:\n",
        "          x1 = self.spectral_convs[i](x)  # Global (Fourier)\n",
        "          x2 = self.conv_layers[i](x)     # Local (1x1 conv)\n",
        "\n",
        "          # Combine and activate\n",
        "          x = x1 + x2\n",
        "          if i < self.n_layers - 1:\n",
        "              x = F.gelu(x)  # GELU activation (smooth ReLU)\n",
        "\n",
        "      # PROJECT back to output dimension\n",
        "      x = x.permute(0, 2, 3, 1)  # [B, H, W, width]\n",
        "      x = self.fc1(x)\n",
        "      x = F.gelu(x)\n",
        "      x = self.fc2(x)  # [B, H, W, out_channels]\n",
        "      x = x.permute(0, 3, 1, 2)  # [B, out_channels, H, W]\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "2BFpRixKFxfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X5F-LbN-FWF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = FNO2d(\n",
        "    modes1=12,          # Keep 12 Fourier modes (captures smooth patterns)\n",
        "    modes2=12,\n",
        "    width=32,           # Hidden dimension\n",
        "    in_channels=2,      # u and v fields\n",
        "    out_channels=2,     # Predict u and v at next timestep\n",
        "    n_layers=4          # 4 Fourier layers\n",
        ").to(device)\n",
        "\n",
        "n_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model parameters: {n_params:,}\")\n",
        "\n",
        "x_test = torch.randn(4, 2, 128, 128).to(device)\n",
        "with torch.no_grad():\n",
        "    y_test = model(x_test)\n",
        "print(f\"Input shape:  {x_test.shape}\")\n",
        "print(f\"Output shape: {y_test.shape}\")\n",
        "print(\"\\nModel initialized successfully!\")"
      ],
      "metadata": {
        "id": "ZiwN6MQMLjwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def train_epoch(model, loader, optimizer, device):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    n_batches = 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: predict next timestep\n",
        "        pred = model(x)\n",
        "\n",
        "        # Loss: Mean squared error between prediction and ground truth\n",
        "        loss = F.mse_loss(pred, y)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        n_batches += 1\n",
        "\n",
        "    return total_loss / n_batches\n",
        "\n",
        "\n",
        "def evaluate(model, loader, device):\n",
        "    \"\"\"Evaluate on test set.\"\"\"\n",
        "    model.eval()\n",
        "    total_mse = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            pred = model(x)\n",
        "\n",
        "            # Sum of squared errors\n",
        "            mse = F.mse_loss(pred, y, reduction='sum')\n",
        "            total_mse += mse.item()\n",
        "            total_samples += x.shape[0] * x.shape[1] * x.shape[2] * x.shape[3]\n",
        "\n",
        "    # Mean over all elements\n",
        "    return total_mse / total_samples\n"
      ],
      "metadata": {
        "id": "GUdt6SgSLpRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "learning_rate = 1e-3\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n"
      ],
      "metadata": {
        "id": "v9YVJeByP0SU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"Training FNO on 2D Diffusion-Reaction\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Epochs: {epochs}\")\n",
        "print(f\"Learning rate: {learning_rate}\")\n",
        "print(f\"Train samples: {len(train_dataset):,}\")\n",
        "print(f\"Test samples: {len(test_dataset):,}\")\n",
        "print(f\"Estimated time: ~{1.7 * epochs:.0f} minutes\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "best_test_loss = float('inf')\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    # Train\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
        "    scheduler.step()\n",
        "\n",
        "    epoch_time = time.time() - epoch_start\n",
        "\n",
        "    # Evaluate every 5 epochs (faster training)\n",
        "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "        test_loss = evaluate(model, test_loader, device)\n",
        "        test_losses.append((epoch, test_loss))\n",
        "\n",
        "        # Save best model\n",
        "        if test_loss < best_test_loss:\n",
        "            best_test_loss = test_loss\n",
        "            torch.save(model.state_dict(), 'best_fno_model.pt')\n",
        "\n",
        "        print(f\"Epoch {epoch+1:3d}/{epochs} ({epoch_time:.1f}s) | Train: {train_loss:.6f} | Test: {test_loss:.6f} | Best: {best_test_loss:.6f}\")\n",
        "    else:\n",
        "        print(f\"Epoch {epoch+1:3d}/{epochs} ({epoch_time:.1f}s) | Train: {train_loss:.6f}\")\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(\"=\"*60)\n",
        "print(f\"Training complete in {total_time/60:.1f} minutes\")\n",
        "print(f\"Best Test Loss: {best_test_loss:.6f}\")\n",
        "print(f\"Best Test RMSE: {np.sqrt(best_test_loss):.6f}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss', alpha=0.7)\n",
        "test_epochs, test_vals = zip(*test_losses)\n",
        "plt.scatter(test_epochs, test_vals, c='red', label='Test Loss', zorder=5)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.legend()\n",
        "plt.title('Training Progress')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_losses, label='Train Loss', alpha=0.7)\n",
        "plt.scatter(test_epochs, test_vals, c='red', label='Test Loss', zorder=5)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss (log scale)')\n",
        "plt.yscale('log')\n",
        "plt.legend()\n",
        "plt.title('Training Progress (Log Scale)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_curves.png', dpi=150)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uNTsLr3kQL0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3BMZo-OnYSZU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}